{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb91b09a",
   "metadata": {},
   "source": [
    "# Comparing Bi-Encoders and Cross-Encoders on NFCorpus Dataset\n",
    "**TAS-B Bi-Encoder**\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load the NFCorpus dataset (corpus, queries, qrels)\n",
    "2. Perform bi-encoder retrieval using FAISS HNSW.\n",
    "3. Save or inspect intermediate variables (embeddings, top-k results).\n",
    "4. Rerank the top-k results with a cross-encoder.\n",
    "5. Compare the evaluations at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbfef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\r\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\r\n",
      "0.00s - to python to disable frozen modules.\r\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\r\n",
      "Available kernels:\r\n",
      "  wse-env    /home/dl5214/.local/share/jupyter/kernels/wse-env\r\n",
      "  python3    /home/shaoyu/anaconda3/share/jupyter/kernels/python3\r\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Select the correct kernel\n",
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fa422",
   "metadata": {},
   "source": [
    "# Part I: Preparations\n",
    "1. Import packages.\n",
    "2. Load data.\n",
    "3. Prepare the queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc935e3",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Logging\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import faiss\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, InputExample\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59cb8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090e7adc",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading Functions\n",
    "def load_scifact_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact corpus from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of document IDs and document texts.\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    doc_texts = []\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            doc_id = data[\"_id\"]\n",
    "            text_str = data.get(\"text\", \"\")\n",
    "            doc_ids.append(doc_id)\n",
    "            doc_texts.append(text_str)\n",
    "    return doc_ids, doc_texts\n",
    "\n",
    "def load_scifact_queries(queries_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact queries from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of query IDs and query texts.\n",
    "    \"\"\"\n",
    "    q_ids = []\n",
    "    q_texts = []\n",
    "    with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            q_ids.append(data[\"_id\"])\n",
    "            q_texts.append(data.get(\"text\", \"\"))\n",
    "    return q_ids, q_texts\n",
    "\n",
    "def load_qrels(qrels_file):\n",
    "    \"\"\"\n",
    "    Loads the qrels from a TSV file.\n",
    "    The first line is a header: 'query-id    corpus-id    score'.\n",
    "    Returns a dictionary: qrels[query_id][doc_id] = relevance_score.\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    with open(qrels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Skip the header line:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            qid, did, rel = line.split()\n",
    "            rel = int(rel)\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][did] = rel\n",
    "    return qrels\n",
    "\n",
    "print('Data loading functions ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e456f24f",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Metric Evaluation Function\n",
    "# We'll define a single function we can call multiple times,\n",
    "# whether for the bi-encoder alone or after cross-encoder reranking.\n",
    "\n",
    "def evaluate_results(qrels, results, k_values=None):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval results using pytrec_eval.\n",
    "    results: dict of dict => results[qid][doc_id] = score\n",
    "    qrels: dict of dict => qrels[qid][doc_id] = relevance\n",
    "    k_values: list of cutoff values (e.g. [5,10,20])\n",
    "    \"\"\"\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"map\", \"ndcg_cut\", \"recall\", \"P\"})\n",
    "    scores = evaluator.evaluate(results)\n",
    "\n",
    "    if k_values is None:\n",
    "        # If k_values not specified, use typical cutoffs\n",
    "        k_values = [5, 10, 20, 30, 100]\n",
    "\n",
    "    ndcg_res = {}\n",
    "    recall_res = {}\n",
    "    prec_res = {}\n",
    "    mrr_vals = []\n",
    "\n",
    "    # Compute mean average precision (MAP) across all queries\n",
    "    map_vals = [scores[qid][\"map\"] for qid in scores]\n",
    "    map_res = np.mean(map_vals)\n",
    "\n",
    "    # Compute NDCG@k, Recall@k, P@k\n",
    "    for k in k_values:\n",
    "        ndcg_vals = []\n",
    "        recall_vals = []\n",
    "        prec_vals = []\n",
    "        for qid in scores:\n",
    "            ndcg_key = f\"ndcg_cut_{k}\"\n",
    "            recall_key = f\"recall_{k}\"\n",
    "            prec_key = f\"P_{k}\"\n",
    "            # If the metric isn't found for some reason, skip\n",
    "            if ndcg_key in scores[qid]:\n",
    "                ndcg_vals.append(scores[qid][ndcg_key])\n",
    "            if recall_key in scores[qid]:\n",
    "                recall_vals.append(scores[qid][recall_key])\n",
    "            if prec_key in scores[qid]:\n",
    "                prec_vals.append(scores[qid][prec_key])\n",
    "\n",
    "        ndcg_res[f\"NDCG@{k}\"] = np.mean(ndcg_vals) if ndcg_vals else 0.0\n",
    "        recall_res[f\"Recall@{k}\"] = np.mean(recall_vals) if recall_vals else 0.0\n",
    "        prec_res[f\"P@{k}\"] = np.mean(prec_vals) if prec_vals else 0.0\n",
    "\n",
    "    # Compute MRR (Mean Reciprocal Rank)\n",
    "    for qid in results:\n",
    "        ranked_docs = sorted(results[qid].items(), key=lambda x: x[1], reverse=True)\n",
    "        for rank, (doc_id, _) in enumerate(ranked_docs, start=1):\n",
    "            if qrels.get(qid, {}).get(doc_id, 0) > 0:  # Relevant document\n",
    "                mrr_vals.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            mrr_vals.append(0)  # No relevant document found in results\n",
    "\n",
    "    mrr_res = np.mean(mrr_vals)\n",
    "\n",
    "    return ndcg_res, map_res, recall_res, prec_res, mrr_res\n",
    "\n",
    "print('Evaluation function ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021a8164",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3633 documents and 3237 queries.\n",
      "Unique qrels: 323\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load the SciFact Data\n",
    "corpus_file = \"datasets/nfcorpus/corpus.jsonl\"\n",
    "queries_file = \"datasets/nfcorpus/queries.jsonl\"\n",
    "qrels_file = \"datasets/nfcorpus/qrels/test.tsv\"\n",
    "\n",
    "doc_ids, doc_texts = load_scifact_corpus(corpus_file)\n",
    "query_ids, query_texts = load_scifact_queries(queries_file)\n",
    "qrels = load_qrels(qrels_file)\n",
    "\n",
    "print(f\"Loaded {len(doc_ids)} documents and {len(query_ids)} queries.\")\n",
    "print(f\"Unique qrels: {len(qrels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef59d33",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered queries for evaluation: 323\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Filter queries to those that appear in qrels\n",
    "filtered_query_ids = [qid for qid in query_ids if qid in qrels]\n",
    "filtered_query_texts = [query_texts[query_ids.index(qid)] for qid in filtered_query_ids]\n",
    "\n",
    "print(f\"Filtered queries for evaluation: {len(filtered_query_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1f349",
   "metadata": {},
   "source": [
    "# Part II: Bi-Encoder Retrieval with FAISS HNSW\n",
    "\n",
    "We'll:\n",
    "1. Load a SentenceTransformer (E5) model.\n",
    "2. Encode all documents with prefix **\"passage: \"**.\n",
    "3. Build a FAISS HNSW index.\n",
    "4. Encode queries with prefix **\"query: \"**.\n",
    "5. Perform approximate nearest neighbor (ANN) search.\n",
    "6. Evaluate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9c53ed",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Bi-Encoder model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Bi-Encoder Setup\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# Set the device to \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "device = \"cuda\" if cuda_available else \"cpu\"\n",
    "\n",
    "# Load the Bi-Encoder model on the specified device\n",
    "bi_model = SentenceTransformer(\"msmarco-distilbert-base-tas-b\", device=device)\n",
    "print(f\"Bi-Encoder model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15acb6a9",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b74b6c9bf740d4937945399c066501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus embeddings shape: (3633, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Encode Corpus with TAS-B\n",
    "# We'll store these embeddings in a variable so we can reuse or inspect them.\n",
    "corpus_embeddings = bi_model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(\"Corpus embeddings shape:\", corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2168fbeb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building HNSW index...\n",
      "INFO:root:Moving HNSW index to GPU...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW index ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Build the FAISS HNSW Index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "M = 15  # number of connections per node in HNSW\n",
    "efConstruction = 300\n",
    "efSearch = 1000\n",
    "\n",
    "logging.info(\"Building HNSW index...\")\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, M)\n",
    "index_hnsw.hnsw.efConstruction = efConstruction\n",
    "index_hnsw.hnsw.efSearch = efSearch\n",
    "\n",
    "# Add corpus embeddings to index\n",
    "index_hnsw.add(corpus_embeddings)\n",
    "\n",
    "# If GPU available, move index to GPU\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(\"Moving HNSW index to GPU...\")\n",
    "    faiss_res = faiss.StandardGpuResources()\n",
    "    index_hnsw = faiss.index_cpu_to_gpu(faiss_res, 0, index_hnsw)\n",
    "\n",
    "print(\"HNSW index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275bd6fb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49cb58d21274a94b29995dd9fc384c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings shape: (323, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Encode Queries with TAS-B\n",
    "# We'll store query embeddings so we can re-run or examine.\n",
    "\n",
    "query_embeddings = bi_model.encode(\n",
    "    filtered_query_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(\"Query embeddings shape:\", query_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36832a0",
   "metadata": {},
   "source": [
    "## Perform ANN Search and Evaluate (Bi-Encoder Only)\n",
    "\n",
    "We'll retrieve the top-k docs from FAISS for each query, then convert them to a `results` dict for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3f9a84",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching query embeddings in HNSW index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-Encoder top-k results ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Bi-Encoder ANN Search\n",
    "top_k = 500\n",
    "logging.info(\"Searching query embeddings in HNSW index...\")\n",
    "D, I = index_hnsw.search(query_embeddings, top_k)\n",
    "\n",
    "# Convert results to pytrec_eval format\n",
    "# results[qid][docid] = score\n",
    "bi_encoder_results = {}\n",
    "for q_idx, qid in enumerate(filtered_query_ids):\n",
    "    bi_encoder_results[qid] = {}\n",
    "    for rank in range(top_k):\n",
    "        doc_idx = I[q_idx, rank]\n",
    "        # for a distance D[q_idx, rank], we can invert or just use negative distance\n",
    "        # Some prefer 1/dist, or -dist, or use it directly.\n",
    "        # We'll do a small trick: score = 1 / distance.\n",
    "        # If distance is 0, we can handle that or set a high value.\n",
    "        dist = D[q_idx, rank]\n",
    "        score = 1/float(dist) if dist != 0 else 1e9\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        bi_encoder_results[qid][doc_id] = score\n",
    "\n",
    "print(\"Bi-Encoder top-k results ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518b7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Results ===\n",
      "Query ID: PLAIN-2\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'MED-10': 3.042715626366151, 'MED-2429': 2.8025310152433374, 'MED-2439': 2.78169012566604\n",
      "--------------------------------------------------\n",
      "Query ID: PLAIN-12\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'MED-2519': 2.060865946861249, 'MED-2514': 2.0394279051095356, 'MED-1928': 2.005049787385\n",
      "--------------------------------------------------\n",
      "Query ID: PLAIN-23\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'MED-1961': 2.648593253886031, 'MED-1169': 2.257566035513047, 'MED-5088': 2.2232102440477\n",
      "--------------------------------------------------\n",
      "Query ID: PLAIN-33\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'MED-2723': 2.8594476924407637, 'MED-2489': 2.6964898428889374, 'MED-5006': 2.60171977712\n",
      "--------------------------------------------------\n",
      "Query ID: PLAIN-44\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'MED-2240': 2.568471889128318, 'MED-2791': 2.5100226440714177, 'MED-1947': 2.474727336556\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the number of retrieved documents for the first 5 queries along with their scores\n",
    "print(\"=== Query Results ===\")\n",
    "for query_id, doc_scores in list(bi_encoder_results.items())[:5]:  # Limit to first 5 queries\n",
    "    num_docs = len(doc_scores)  # Number of documents retrieved for this query\n",
    "    print(f\"Query ID: {query_id}\")\n",
    "    print(f\"Number of retrieved documents: {num_docs}\")\n",
    "    print(f\"Scores (truncated): {str(doc_scores)[:90]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0376a3a",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bi-Encoder (E5) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.2810\n",
      "  NDCG@10: 0.2564\n",
      "  NDCG@20: 0.2360\n",
      "  NDCG@30: 0.2285\n",
      "  NDCG@100: 0.2303\n",
      "\n",
      "MAP: 0.1132\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.0917\n",
      "  Recall@10: 0.1197\n",
      "  Recall@20: 0.1507\n",
      "  Recall@30: 0.1678\n",
      "  Recall@100: 0.2338\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.2446\n",
      "  P@10    : 0.1938\n",
      "  P@20    : 0.1399\n",
      "  P@30    : 0.1134\n",
      "  P@100   : 0.0593\n",
      "\n",
      "MRR: 0.4441\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Evaluate Bi-Encoder Results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_bi, map_bi, recall_bi, prec_bi, mrr_bi = evaluate_results(qrels, bi_encoder_results, k_values)\n",
    "\n",
    "print(\"=== Bi-Encoder (E5) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_bi)\n",
    "# print(\"MAP:\", map_bi)\n",
    "# print(\"Recall@k:\", recall_bi)\n",
    "# print(\"Precision@k:\", prec_bi)\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_bi.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_bi:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_bi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92333d1d",
   "metadata": {},
   "source": [
    "# Part III: Cross-Encoder Reranking\n",
    "\n",
    "We'll now demonstrate how to:\n",
    "1. Take the **top-k** documents from the **bi-encoder** results.\n",
    "2. Rerank them with a **CrossEncoder**.\n",
    "3. Evaluate again using the same `evaluate_results` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e022653",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder rerank function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define a function to rerank with a CrossEncoder\n",
    "\n",
    "def cross_encode_rerank(cross_encoder_model, \n",
    "                        query_ids_list, query_texts_list,\n",
    "                        topk_results, corpus_text_map,\n",
    "                        rerank_top_k=500, final_top_k=100):\n",
    "    \"\"\"\n",
    "    cross_encoder_model: a CrossEncoder from sentence-transformers\n",
    "    query_ids_list: list of query IDs (strings)\n",
    "    query_texts_list: list of corresponding query texts\n",
    "    topk_results: dict => topk_results[qid][doc_id] = some bi-encoder score\n",
    "    corpus_text_map: a dict doc_id -> doc_text\n",
    "    rerank_top_k: how many documents to consider for reranking from the bi-encoder results\n",
    "    final_top_k: how many documents to keep after cross-encoder reranking\n",
    "\n",
    "    returns: dict => reranked_results[qid][doc_id] = cross-encoder score\n",
    "    \"\"\"\n",
    "    reranked_results = {}\n",
    "\n",
    "    # Use tqdm to show total progress for all queries\n",
    "    with tqdm(total=len(query_ids_list), desc=\"Cross-encoding queries\", unit=\"query\", ncols=80) as pbar:\n",
    "        for idx, qid in enumerate(query_ids_list):\n",
    "            # Get top rerank_top_k document candidates for the query\n",
    "            doc_candidates = sorted(topk_results[qid].items(), key=lambda x: x[1], reverse=True)[:rerank_top_k]\n",
    "            doc_ids = [doc_id for doc_id, _ in doc_candidates]\n",
    "            query_text = query_texts_list[idx]\n",
    "\n",
    "            # Prepare query-document pairs for the cross-encoder\n",
    "#             pair_texts = [(f\"query: {query_text}\", f\"passage: {corpus_text_map[doc_id]}\") for doc_id in doc_ids]\n",
    "            pair_texts = [(query_text, corpus_text_map[doc_id]) for doc_id in doc_ids]\n",
    "            \n",
    "            # Perform cross-encoder inference, ensure no additional progress bars are shown\n",
    "            scores = cross_encoder_model.predict(pair_texts, show_progress_bar=False)\n",
    "\n",
    "            # Combine document IDs with their Cross-Encoder scores\n",
    "            reranked_doc_scores = {doc_ids[i]: float(scores[i]) for i in range(len(doc_ids))}\n",
    "\n",
    "            # Sort reranked results by Cross-Encoder scores and keep final_top_k\n",
    "            reranked_results[qid] = dict(sorted(reranked_doc_scores.items(), key=lambda x: x[1], reverse=True)[:final_top_k])\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "print('Cross-encoder rerank function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc70c7",
   "metadata": {},
   "source": [
    "### Prepare Data Structures for Reranking\n",
    "\n",
    "- We already have `bi_encoder_results`, which is `dict[qid][doc_id] = score`.\n",
    "- We need a quick way to map doc IDs to actual text for the CrossEncoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb81cd38",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id_to_text dictionary has 3633 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Create a dictionary doc_id -> doc_text\n",
    "doc_id_to_text = {}\n",
    "for i, did in enumerate(doc_ids):\n",
    "    doc_id_to_text[did] = doc_texts[i]\n",
    "\n",
    "print(f\"doc_id_to_text dictionary has {len(doc_id_to_text)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951921",
   "metadata": {},
   "source": [
    "## 3.1: MiniLM CrossEncoder Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e642f9",
   "metadata": {},
   "source": [
    "### Load a CrossEncoder\n",
    "Choose any cross-encoder checkpoint from Hugging Face or sentence-transformers.\n",
    "For demonstration, we'll pick something like `'cross-encoder/ms-marco-MiniLM-L-6-v2'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "039567e0",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7e047",
   "metadata": {},
   "source": [
    "### Rerank the top-k results from the Bi-Encoder\n",
    "\n",
    "We'll use the cross-encoder function defined above and pass in the top-k results from the Bi-Encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "238a2dd5",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|██████████████| 323/323 [06:30<00:00,  1.21s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_minilm = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca8420",
   "metadata": {},
   "source": [
    "### Evaluate Cross-Encoder Reranked Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1281eedb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MiniLM Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3715\n",
      "  NDCG@10: 0.3338\n",
      "  NDCG@20: 0.3059\n",
      "  NDCG@30: 0.2931\n",
      "  NDCG@100: 0.2861\n",
      "\n",
      "MAP: 0.1486\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1267\n",
      "  Recall@10: 0.1555\n",
      "  Recall@20: 0.1826\n",
      "  Recall@30: 0.1967\n",
      "  Recall@100: 0.2590\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3121\n",
      "  P@10    : 0.2378\n",
      "  P@20    : 0.1743\n",
      "  P@30    : 0.1404\n",
      "  P@100   : 0.0690\n",
      "\n",
      "MRR: 0.5546\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_minilm, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== MiniLM Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a532381f",
   "metadata": {},
   "source": [
    "## 3.2: Electra CrossEncoder Reranking\n",
    "To compare the performance of different cross-encoders, here we use `'cross-encoder/ms-marco-electra-base'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "127e8f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-electra-base\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75bcb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|██████████████| 323/323 [46:06<00:00,  8.57s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_electra = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91efbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Electra Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3168\n",
      "  NDCG@10: 0.2873\n",
      "  NDCG@20: 0.2596\n",
      "  NDCG@30: 0.2515\n",
      "  NDCG@100: 0.2535\n",
      "\n",
      "MAP: 0.1223\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1104\n",
      "  Recall@10: 0.1363\n",
      "  Recall@20: 0.1593\n",
      "  Recall@30: 0.1785\n",
      "  Recall@100: 0.2468\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.2762\n",
      "  P@10    : 0.2130\n",
      "  P@20    : 0.1471\n",
      "  P@30    : 0.1200\n",
      "  P@100   : 0.0625\n",
      "\n",
      "MRR: 0.4823\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_electra, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Electra Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d9618",
   "metadata": {},
   "source": [
    "## 3.3: Tiny-BERT CrossEncoder Reranking\n",
    "To compare the performance of different cross-encoders, here we use `'cross-encoder/ms-marco-TinyBERT-L-2-v2'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_tinybert = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ad71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_tinybert, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Tiny-BERT Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a776db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15382c11",
   "metadata": {},
   "source": [
    "# Part IV: Fine Tuning Cross-Encoders\n",
    "Below demonstrates the process of fine-tuning Cross-Encoders on query-document pairs for relevance scoring. It includes functions to load training data, prepare input examples, and fine-tune a Cross-Encoder model using training data. The fine-tuned model can then be used for downstream reranking tasks or other applications requiring precise query-document matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea6a9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder fine-tuning functions ready.\n"
     ]
    }
   ],
   "source": [
    "def build_cross_encoder_input_examples(qrels, query_dict, doc_dict, neg_ratio=10):\n",
    "    \"\"\"\n",
    "    Converts qrels and additional negative samples to InputExample objects for CrossEncoder training.\n",
    "    Ensures the inclusion of both positive and negative samples.\n",
    "\n",
    "    qrels: dict of dict => qrels[qid][did] = relevance_score (e.g., 1 for positive samples)\n",
    "    query_dict: dict of query_id -> query_text\n",
    "    doc_dict: dict of doc_id -> doc_text\n",
    "    neg_ratio: Ratio of negative samples to positive samples (e.g., 10 means 10 negatives per positive).\n",
    "\n",
    "    Returns:\n",
    "        examples: List of InputExample\n",
    "        num_pos: Number of positive examples\n",
    "        num_neg: Number of negative examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "\n",
    "    for qid in qrels:\n",
    "        if qid not in query_dict:\n",
    "            continue\n",
    "        \n",
    "        # Positive samples (label=1)\n",
    "        positive_docs = [did for did, score in qrels[qid].items() if score == 1]\n",
    "        for did in positive_docs:\n",
    "            if did in doc_dict:\n",
    "                examples.append(InputExample(texts=[query_dict[qid], doc_dict[did]], label=1))\n",
    "                num_pos += 1\n",
    "        \n",
    "        # Negative samples (label=0)\n",
    "        all_docs = set(doc_dict.keys())\n",
    "        negative_candidates = list(all_docs - set(qrels[qid].keys()))  # Exclude positive and qrels docs\n",
    "        negative_samples = random.sample(negative_candidates, min(len(negative_candidates), len(positive_docs) * neg_ratio))\n",
    "        \n",
    "        for did in negative_samples:\n",
    "            if did in doc_dict:\n",
    "                examples.append(InputExample(texts=[query_dict[qid], doc_dict[did]], label=0))\n",
    "                num_neg += 1\n",
    "\n",
    "    print(f\"Generated {num_pos} positive samples and {num_neg} negative samples.\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def fine_tune_cross_encoder(model_name,\n",
    "                            train_examples,\n",
    "                            output_model_path=\"fine_tuned_cross_encoder\",\n",
    "                            epochs=1,\n",
    "                            batch_size=8,\n",
    "                            lr=1e-5,\n",
    "                            warmup_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Fine-tunes a CrossEncoder on the given (query, doc, label) examples.\n",
    "    model_name: e.g. \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    train_examples: list of InputExample\n",
    "    output_model_path: where to save the fine-tuned model\n",
    "    epochs, batch_size, lr, warmup_ratio: training parameters\n",
    "    \"\"\"\n",
    "    # Initialize a CrossEncoder with 1 output (for regression or binary classification)\n",
    "    cross_encoder = CrossEncoder(model_name, num_labels=1)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Total steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "    # Fit the model\n",
    "    cross_encoder.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={'lr': lr},\n",
    "    )\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    cross_encoder.save(output_model_path)\n",
    "\n",
    "    # Reload the model from disk to ensure it's saved correctly\n",
    "    fine_tuned_model = CrossEncoder(output_model_path)\n",
    "    return fine_tuned_model\n",
    "\n",
    "print(\"CrossEncoder fine-tuning functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7645ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 110575 positive samples and 938541 negative samples.\n",
      "Number of training examples: 1049116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training Data for CrossEncoder\n",
    "\n",
    "# 1) Training data filepath:\n",
    "train_qrels_file = \"datasets/nfcorpus/qrels/train.tsv\"   # or wherever your train file is\n",
    "train_queries_file = \"datasets/nfcorpus/queries.jsonl\"\n",
    "train_corpus_file = \"datasets/nfcorpus/corpus.jsonl\"\n",
    "\n",
    "# 2) Load queries and docs for training\n",
    "train_qids, train_qtexts = load_scifact_queries(train_queries_file)\n",
    "train_docids, train_doctexts = load_scifact_corpus(train_corpus_file)\n",
    "\n",
    "# Build dictionaries for easy lookup\n",
    "train_query_dict = {q: t for q, t in zip(train_qids, train_qtexts)}\n",
    "train_doc_dict = {d: t for d, t in zip(train_docids, train_doctexts)}\n",
    "\n",
    "# 3) Load the training qrels and build InputExample for cross-encoder\n",
    "train_qrels = load_qrels(train_qrels_file)  # Returns a nested dictionary\n",
    "# train_examples = build_cross_encoder_input_examples(\n",
    "#     [(qid, did, train_qrels[qid][did]) for qid in train_qrels for did in train_qrels[qid]],\n",
    "#     train_query_dict,\n",
    "#     train_doc_dict\n",
    "# )\n",
    "train_examples = build_cross_encoder_input_examples(\n",
    "    qrels=train_qrels,  # Pass the nested dictionary directly\n",
    "    query_dict=train_query_dict,\n",
    "    doc_dict=train_doc_dict,\n",
    "    neg_ratio=10  # Adjust as needed\n",
    ")\n",
    "print(f\"Number of training examples: {len(train_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec9da8",
   "metadata": {},
   "source": [
    "## 4.1 Fine Tuning MiniLM Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1320f7f1700947138059aa2cf33339eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c28ebdbf98497fb7d711208a9aba01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/65570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Fine-Tune the MiniLM CrossEncoder\n",
    "#######################################################\n",
    "# 1) Name of the base cross-encoder\n",
    "base_cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# 2) Output path for the fine-tuned model\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_minilm\"\n",
    "\n",
    "# 3) Fine-tune\n",
    "# Adjust epochs, batch_size, lr, etc. as needed.\n",
    "fine_tuned_cross_encoder = fine_tune_cross_encoder(\n",
    "    model_name=base_cross_model_name,\n",
    "    train_examples=train_examples,\n",
    "    output_model_path=fine_tuned_model_path,\n",
    "    epochs=3,        # example\n",
    "    batch_size=16,   # example\n",
    "    lr=2e-5,         # example\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete. Saved to:\", fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8f4ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the fine-tuned cross-encoder...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|██████████████| 323/323 [06:29<00:00,  1.21s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the fine-tuned cross-encoder...\")\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_minilm\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf417bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-Tuned Cross-Encoder (MiniLM) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3933\n",
      "  NDCG@10: 0.3684\n",
      "  NDCG@20: 0.3396\n",
      "  NDCG@30: 0.3265\n",
      "  NDCG@100: 0.3128\n",
      "\n",
      "MAP: 0.1662\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1273\n",
      "  Recall@10: 0.1714\n",
      "  Recall@20: 0.2107\n",
      "  Recall@30: 0.2325\n",
      "  Recall@100: 0.3021\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3548\n",
      "  P@10    : 0.2923\n",
      "  P@20    : 0.2146\n",
      "  P@30    : 0.1737\n",
      "  P@100   : 0.0819\n",
      "\n",
      "MRR: 0.5434\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Fine-Tuned Cross-Encoder (MiniLM) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17146df3",
   "metadata": {},
   "source": [
    "## 4.2 Fine Tuning TinyBERT Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d033d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Fine-Tune the MiniLM CrossEncoder\n",
    "#######################################################\n",
    "# 1) Name of the base cross-encoder\n",
    "base_cross_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    "\n",
    "# 2) Output path for the fine-tuned model\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_tinybert\"\n",
    "\n",
    "# 3) Fine-tune\n",
    "# Adjust epochs, batch_size, lr, etc. as needed.\n",
    "fine_tuned_cross_encoder = fine_tune_cross_encoder(\n",
    "    model_name=base_cross_model_name,\n",
    "    train_examples=train_examples,\n",
    "    output_model_path=fine_tuned_model_path,\n",
    "    epochs=3,        # example\n",
    "    batch_size=16,   # example\n",
    "    lr=2e-5,         # example\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete. Saved to:\", fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the fine-tuned cross-encoder...\")\n",
    "fine_tuned_model_path = \"./models/scifact_fine_tuned_cross_encoder_tinybert\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Fine-Tuned Cross-Encoder (Tiny-BERT) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bd694",
   "metadata": {},
   "source": [
    "## 4.3 Fine Tuning MiniLM Cross Encoder (Limited Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3562f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cross_encoder_limited_examples(qrels, query_dict, doc_dict, pos_limit=2000, neg_limit=20000):\n",
    "    \"\"\"\n",
    "    Converts qrels and additional limited negative samples to InputExample objects for CrossEncoder training.\n",
    "    Ensures the inclusion of both positive and negative samples with given limits.\n",
    "\n",
    "    qrels: dict of dict => qrels[qid][did] = relevance_score (e.g., 1 for positive samples)\n",
    "    query_dict: dict of query_id -> query_text\n",
    "    doc_dict: dict of doc_id -> doc_text\n",
    "    pos_limit: Limit on the number of positive samples (e.g., 2000).\n",
    "    neg_limit: Limit on the number of negative samples (e.g., 20000).\n",
    "\n",
    "    Returns:\n",
    "        examples: List of InputExample\n",
    "        num_pos: Number of positive examples\n",
    "        num_neg: Number of negative examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "\n",
    "    all_positive_samples = []\n",
    "    all_negative_candidates = []\n",
    "\n",
    "    # Gather all positive and negative candidates\n",
    "    for qid in qrels:\n",
    "        if qid not in query_dict:\n",
    "            continue\n",
    "\n",
    "        # Positive samples (label=1)\n",
    "        positive_docs = [did for did, score in qrels[qid].items() if score == 1]\n",
    "        for did in positive_docs:\n",
    "            if did in doc_dict:\n",
    "                all_positive_samples.append((qid, did, 1))\n",
    "\n",
    "        # Negative candidates (not in qrels or positive docs)\n",
    "        all_docs = set(doc_dict.keys())\n",
    "        negative_candidates = list(all_docs - set(qrels[qid].keys()))\n",
    "        for did in negative_candidates:\n",
    "            if did in doc_dict:\n",
    "                all_negative_candidates.append((qid, did, 0))\n",
    "\n",
    "    # Randomly sample positives and negatives\n",
    "    selected_positives = random.sample(all_positive_samples, min(len(all_positive_samples), pos_limit))\n",
    "    selected_negatives = random.sample(all_negative_candidates, min(len(all_negative_candidates), neg_limit))\n",
    "\n",
    "    # Build InputExample for positive samples\n",
    "    for qid, did, label in selected_positives:\n",
    "        examples.append(InputExample(texts=[query_dict[qid], doc_dict[did]], label=label))\n",
    "        num_pos += 1\n",
    "\n",
    "    # Build InputExample for negative samples\n",
    "    for qid, did, label in selected_negatives:\n",
    "        examples.append(InputExample(texts=[query_dict[qid], doc_dict[did]], label=label))\n",
    "        num_neg += 1\n",
    "\n",
    "    print(f\"Generated {num_pos} positive samples and {num_neg} negative samples (limited).\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training Data for CrossEncoder\n",
    "\n",
    "# 1) Training data filepath:\n",
    "train_qrels_file = \"datasets/nfcorpus/qrels/train.tsv\"   # or wherever your train file is\n",
    "train_queries_file = \"datasets/nfcorpus/queries.jsonl\"\n",
    "train_corpus_file = \"datasets/nfcorpus/corpus.jsonl\"\n",
    "\n",
    "# 2) Load queries and docs for training\n",
    "train_qids, train_qtexts = load_scifact_queries(train_queries_file)\n",
    "train_docids, train_doctexts = load_scifact_corpus(train_corpus_file)\n",
    "\n",
    "# Build dictionaries for easy lookup\n",
    "train_query_dict = {q: t for q, t in zip(train_qids, train_qtexts)}\n",
    "train_doc_dict = {d: t for d, t in zip(train_docids, train_doctexts)}\n",
    "\n",
    "# 3) Load the training qrels and build InputExample for cross-encoder\n",
    "train_qrels = load_qrels(train_qrels_file)  # Returns a nested dictionary\n",
    "# train_examples = build_cross_encoder_input_examples(\n",
    "#     [(qid, did, train_qrels[qid][did]) for qid in train_qrels for did in train_qrels[qid]],\n",
    "#     train_query_dict,\n",
    "#     train_doc_dict\n",
    "# )\n",
    "train_examples = build_cross_encoder_limited_examples(\n",
    "    qrels=train_qrels,  # Pass the nested dictionary directly\n",
    "    query_dict=train_query_dict,\n",
    "    doc_dict=train_doc_dict\n",
    ")\n",
    "print(f\"Number of training examples: {len(train_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Fine-Tune the MiniLM CrossEncoder\n",
    "#######################################################\n",
    "# 1) Name of the base cross-encoder\n",
    "base_cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# 2) Output path for the fine-tuned model\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_minilm_limited\"\n",
    "\n",
    "# 3) Fine-tune\n",
    "# Adjust epochs, batch_size, lr, etc. as needed.\n",
    "fine_tuned_cross_encoder = fine_tune_cross_encoder(\n",
    "    model_name=base_cross_model_name,\n",
    "    train_examples=train_examples,\n",
    "    output_model_path=fine_tuned_model_path,\n",
    "    epochs=3,        # example\n",
    "    batch_size=16,   # example\n",
    "    lr=2e-5,         # example\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete. Saved to:\", fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da62e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the fine-tuned cross-encoder...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|██████████████| 323/323 [06:35<00:00,  1.22s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the fine-tuned cross-encoder...\")\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_minilm_limited\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edf41056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-Tuned Cross-Encoder (MiniLM, Limited) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3808\n",
      "  NDCG@10: 0.3450\n",
      "  NDCG@20: 0.3163\n",
      "  NDCG@30: 0.3038\n",
      "  NDCG@100: 0.2955\n",
      "\n",
      "MAP: 0.1560\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1299\n",
      "  Recall@10: 0.1609\n",
      "  Recall@20: 0.1890\n",
      "  Recall@30: 0.2078\n",
      "  Recall@100: 0.2713\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3276\n",
      "  P@10    : 0.2505\n",
      "  P@20    : 0.1842\n",
      "  P@30    : 0.1478\n",
      "  P@100   : 0.0722\n",
      "\n",
      "MRR: 0.5586\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Fine-Tuned Cross-Encoder (MiniLM, Limited) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8ad84",
   "metadata": {},
   "source": [
    "## 4.4 Fine Tuning TinyBERT Cross Encoder (Limited Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efae6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Fine-Tune the MiniLM CrossEncoder\n",
    "#######################################################\n",
    "# 1) Name of the base cross-encoder\n",
    "base_cross_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    "\n",
    "# 2) Output path for the fine-tuned model\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_tinybert_limited\"\n",
    "\n",
    "# 3) Fine-tune\n",
    "# Adjust epochs, batch_size, lr, etc. as needed.\n",
    "fine_tuned_cross_encoder = fine_tune_cross_encoder(\n",
    "    model_name=base_cross_model_name,\n",
    "    train_examples=train_examples,\n",
    "    output_model_path=fine_tuned_model_path,\n",
    "    epochs=3,        # example\n",
    "    batch_size=16,   # example\n",
    "    lr=2e-5,         # example\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete. Saved to:\", fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20ffd3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the fine-tuned cross-encoder...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|██████████████| 323/323 [00:36<00:00,  8.75query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the fine-tuned cross-encoder...\")\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_tinybert_limited\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9e4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-Tuned Cross-Encoder (Tiny-BERT, Limited) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3549\n",
      "  NDCG@10: 0.3192\n",
      "  NDCG@20: 0.2917\n",
      "  NDCG@30: 0.2801\n",
      "  NDCG@100: 0.2757\n",
      "\n",
      "MAP: 0.1409\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1224\n",
      "  Recall@10: 0.1538\n",
      "  Recall@20: 0.1798\n",
      "  Recall@30: 0.1929\n",
      "  Recall@100: 0.2562\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3046\n",
      "  P@10    : 0.2307\n",
      "  P@20    : 0.1667\n",
      "  P@30    : 0.1358\n",
      "  P@100   : 0.0684\n",
      "\n",
      "MRR: 0.5326\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Fine-Tuned Cross-Encoder (Tiny-BERT, Limited) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wse-env)",
   "language": "python",
   "name": "wse-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

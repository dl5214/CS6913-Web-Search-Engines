{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb91b09a",
   "metadata": {},
   "source": [
    "# Comparing Bi-Encoders and Cross-Encoders on Scifact Dataset\n",
    "**E5 Bi-Encoder, NFCorpus**\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load the nfcorpus dataset (corpus, queries, qrels)\n",
    "2. Perform bi-encoder retrieval using FAISS HNSW.\n",
    "3. Save or inspect intermediate variables (embeddings, top-k results).\n",
    "4. Rerank the top-k results with a cross-encoder.\n",
    "5. Compare the evaluations at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbfef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\r\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\r\n",
      "0.00s - to python to disable frozen modules.\r\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\r\n",
      "Available kernels:\r\n",
      "  wse-env    /home/dl5214/.local/share/jupyter/kernels/wse-env\r\n",
      "  python3    /home/shaoyu/anaconda3/share/jupyter/kernels/python3\r\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Select the correct kernel\n",
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fa422",
   "metadata": {},
   "source": [
    "# Part I: Preparations\n",
    "1. Import packages.\n",
    "2. Load data.\n",
    "3. Prepare the queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc935e3",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Logging\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import faiss\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59cb8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090e7adc",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading Functions\n",
    "def load_scifact_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact corpus from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of document IDs and document texts.\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    doc_texts = []\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            doc_id = data[\"_id\"]\n",
    "            text_str = data.get(\"text\", \"\")\n",
    "            doc_ids.append(doc_id)\n",
    "            doc_texts.append(text_str)\n",
    "    return doc_ids, doc_texts\n",
    "\n",
    "def load_scifact_queries(queries_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact queries from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of query IDs and query texts.\n",
    "    \"\"\"\n",
    "    q_ids = []\n",
    "    q_texts = []\n",
    "    with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            q_ids.append(data[\"_id\"])\n",
    "            q_texts.append(data.get(\"text\", \"\"))\n",
    "    return q_ids, q_texts\n",
    "\n",
    "def load_qrels(qrels_file):\n",
    "    \"\"\"\n",
    "    Loads the qrels from a TSV file.\n",
    "    The first line is a header: 'query-id    corpus-id    score'.\n",
    "    Returns a dictionary: qrels[query_id][doc_id] = relevance_score.\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    with open(qrels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Skip the header line:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            qid, did, rel = line.split()\n",
    "            rel = int(rel)\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][did] = rel\n",
    "    return qrels\n",
    "\n",
    "print('Data loading functions ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e456f24f",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Metric Evaluation Function\n",
    "# We'll define a single function we can call multiple times,\n",
    "# whether for the bi-encoder alone or after cross-encoder reranking.\n",
    "\n",
    "def evaluate_results(qrels, results, k_values=None):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval results using pytrec_eval.\n",
    "    results: dict of dict => results[qid][doc_id] = score\n",
    "    qrels: dict of dict => qrels[qid][doc_id] = relevance\n",
    "    k_values: list of cutoff values (e.g. [5,10,20])\n",
    "    \"\"\"\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"map\", \"ndcg_cut\", \"recall\", \"P\"})\n",
    "    scores = evaluator.evaluate(results)\n",
    "\n",
    "    if k_values is None:\n",
    "        # If k_values not specified, use typical cutoffs\n",
    "        k_values = [5, 10, 20, 30, 100]\n",
    "\n",
    "    ndcg_res = {}\n",
    "    recall_res = {}\n",
    "    prec_res = {}\n",
    "    mrr_vals = []\n",
    "\n",
    "    # Compute mean average precision (MAP) across all queries\n",
    "    map_vals = [scores[qid][\"map\"] for qid in scores]\n",
    "    map_res = np.mean(map_vals)\n",
    "\n",
    "    # Compute NDCG@k, Recall@k, P@k\n",
    "    for k in k_values:\n",
    "        ndcg_vals = []\n",
    "        recall_vals = []\n",
    "        prec_vals = []\n",
    "        for qid in scores:\n",
    "            ndcg_key = f\"ndcg_cut_{k}\"\n",
    "            recall_key = f\"recall_{k}\"\n",
    "            prec_key = f\"P_{k}\"\n",
    "            # If the metric isn't found for some reason, skip\n",
    "            if ndcg_key in scores[qid]:\n",
    "                ndcg_vals.append(scores[qid][ndcg_key])\n",
    "            if recall_key in scores[qid]:\n",
    "                recall_vals.append(scores[qid][recall_key])\n",
    "            if prec_key in scores[qid]:\n",
    "                prec_vals.append(scores[qid][prec_key])\n",
    "\n",
    "        ndcg_res[f\"NDCG@{k}\"] = np.mean(ndcg_vals) if ndcg_vals else 0.0\n",
    "        recall_res[f\"Recall@{k}\"] = np.mean(recall_vals) if recall_vals else 0.0\n",
    "        prec_res[f\"P@{k}\"] = np.mean(prec_vals) if prec_vals else 0.0\n",
    "\n",
    "    # Compute MRR (Mean Reciprocal Rank)\n",
    "    for qid in results:\n",
    "        ranked_docs = sorted(results[qid].items(), key=lambda x: x[1], reverse=True)\n",
    "        for rank, (doc_id, _) in enumerate(ranked_docs, start=1):\n",
    "            if qrels.get(qid, {}).get(doc_id, 0) > 0:  # Relevant document\n",
    "                mrr_vals.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            mrr_vals.append(0)  # No relevant document found in results\n",
    "\n",
    "    mrr_res = np.mean(mrr_vals)\n",
    "\n",
    "    return ndcg_res, map_res, recall_res, prec_res, mrr_res\n",
    "\n",
    "print('Evaluation function ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021a8164",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 382545 documents and 49 queries.\n",
      "Unique qrels: 49\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load the SciFact Data\n",
    "corpus_file = \"datasets/webis-touche2020/corpus.jsonl\"\n",
    "queries_file = \"datasets/webis-touche2020/queries.jsonl\"\n",
    "qrels_file = \"datasets/webis-touche2020/qrels/test.tsv\"\n",
    "\n",
    "doc_ids, doc_texts = load_scifact_corpus(corpus_file)\n",
    "query_ids, query_texts = load_scifact_queries(queries_file)\n",
    "qrels = load_qrels(qrels_file)\n",
    "\n",
    "print(f\"Loaded {len(doc_ids)} documents and {len(query_ids)} queries.\")\n",
    "print(f\"Unique qrels: {len(qrels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef59d33",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered queries for evaluation: 49\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Filter queries to those that appear in qrels\n",
    "filtered_query_ids = [qid for qid in query_ids if qid in qrels]\n",
    "filtered_query_texts = [query_texts[query_ids.index(qid)] for qid in filtered_query_ids]\n",
    "\n",
    "print(f\"Filtered queries for evaluation: {len(filtered_query_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1f349",
   "metadata": {},
   "source": [
    "# Part II: Bi-Encoder Retrieval with FAISS HNSW\n",
    "\n",
    "We'll:\n",
    "1. Load a SentenceTransformer (E5) model.\n",
    "2. Encode all documents with prefix **\"passage: \"**.\n",
    "3. Build a FAISS HNSW index.\n",
    "4. Encode queries with prefix **\"query: \"**.\n",
    "5. Perform approximate nearest neighbor (ANN) search.\n",
    "6. Evaluate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9c53ed",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/e5-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Bi-Encoder model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Bi-Encoder Setup\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# Set the device to \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "device = \"cuda\" if cuda_available else \"cpu\"\n",
    "\n",
    "# Load the Bi-Encoder model on the specified device\n",
    "bi_model = SentenceTransformer(\"intfloat/e5-base-v2\", device=device)\n",
    "print(f\"Bi-Encoder model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15acb6a9",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652c438ef4a84ce3844c5511dc3d0d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus embeddings shape: (382545, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Encode Corpus with E5 (prefix 'passage: ')\n",
    "# We'll store these embeddings in a variable so we can reuse or inspect them.\n",
    "corpus_embeddings = bi_model.encode([\n",
    "    \"passage: \" + text for text in doc_texts\n",
    "],\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(\"Corpus embeddings shape:\", corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861e5f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus embeddings saved in HDF5 format to: ./embeddings/e5_corpus_embeddings_touche2020.h5\n"
     ]
    }
   ],
   "source": [
    "# Directory path to save embeddings\n",
    "embedding_dir = \"./embeddings\"\n",
    "os.makedirs(embedding_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save the embeddings as an HDF5 file\n",
    "hdf5_file = os.path.join(embedding_dir, \"e5_corpus_embeddings_touche2020.h5\")\n",
    "with h5py.File(hdf5_file, \"w\") as f:\n",
    "    # Use gzip compression to reduce file size\n",
    "    f.create_dataset(\"embeddings\", data=corpus_embeddings, compression=\"gzip\")\n",
    "    f.create_dataset(\"shape\", data=corpus_embeddings.shape)  # Store shape for later use\n",
    "\n",
    "print(f\"Corpus embeddings saved in HDF5 format to: {hdf5_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df51f275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus embeddings loaded.\n",
      "Shape of loaded embeddings: (382545, 768)\n"
     ]
    }
   ],
   "source": [
    "# Filepath to the saved embeddings\n",
    "embedding_dir = \"./embeddings\"\n",
    "hdf5_file = os.path.join(embedding_dir, \"e5_corpus_embeddings_touche2020.h5\")\n",
    "\n",
    "# Load the embeddings from the HDF5 file\n",
    "with h5py.File(hdf5_file, \"r\") as f:\n",
    "    corpus_embeddings = f[\"embeddings\"][:]  # Load the embeddings dataset\n",
    "    embeddings_shape = tuple(f[\"shape\"][:])  # Load the shape\n",
    "\n",
    "print(\"Corpus embeddings loaded.\")\n",
    "print(f\"Shape of loaded embeddings: {embeddings_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2168fbeb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building HNSW index...\n",
      "INFO:root:Moving HNSW index to GPU...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW index ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Build the FAISS HNSW Index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "M = 15  # number of connections per node in HNSW\n",
    "efConstruction = 300\n",
    "efSearch = 1000\n",
    "\n",
    "logging.info(\"Building HNSW index...\")\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, M)\n",
    "index_hnsw.hnsw.efConstruction = efConstruction\n",
    "index_hnsw.hnsw.efSearch = efSearch\n",
    "\n",
    "# Add corpus embeddings to index\n",
    "index_hnsw.add(corpus_embeddings)\n",
    "\n",
    "# If GPU available, move index to GPU\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(\"Moving HNSW index to GPU...\")\n",
    "    faiss_res = faiss.StandardGpuResources()\n",
    "    index_hnsw = faiss.index_cpu_to_gpu(faiss_res, 0, index_hnsw)\n",
    "\n",
    "print(\"HNSW index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275bd6fb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e78f70db9d4eb3ae3e1539bfdad6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings shape: (49, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Encode Queries with E5 (prefix 'query: ')\n",
    "# We'll store query embeddings so we can re-run or examine.\n",
    "\n",
    "query_embeddings = bi_model.encode([\n",
    "    \"query: \" + text for text in filtered_query_texts\n",
    "],\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(\"Query embeddings shape:\", query_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36832a0",
   "metadata": {},
   "source": [
    "## Perform ANN Search and Evaluate (Bi-Encoder Only)\n",
    "\n",
    "We'll retrieve the top-k docs from FAISS for each query, then convert them to a `results` dict for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3f9a84",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching query embeddings in HNSW index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-Encoder top-k results ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Bi-Encoder ANN Search\n",
    "top_k = 500\n",
    "logging.info(\"Searching query embeddings in HNSW index...\")\n",
    "D, I = index_hnsw.search(query_embeddings, top_k)\n",
    "\n",
    "# Convert results to pytrec_eval format\n",
    "# results[qid][docid] = score\n",
    "bi_encoder_results = {}\n",
    "for q_idx, qid in enumerate(filtered_query_ids):\n",
    "    bi_encoder_results[qid] = {}\n",
    "    for rank in range(top_k):\n",
    "        doc_idx = I[q_idx, rank]\n",
    "        # for a distance D[q_idx, rank], we can invert or just use negative distance\n",
    "        # Some prefer 1/dist, or -dist, or use it directly.\n",
    "        # We'll do a small trick: score = 1 / distance.\n",
    "        # If distance is 0, we can handle that or set a high value.\n",
    "        dist = D[q_idx, rank]\n",
    "        score = 1/float(dist) if dist != 0 else 1e9\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        bi_encoder_results[qid][doc_id] = score\n",
    "\n",
    "print(\"Bi-Encoder top-k results ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518b7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Results ===\n",
      "Query ID: 1\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'c065954f-2019-04-18T14:32:52Z-00002-000': 4.486143324778476, '4d8487a-2019-04-18T18:20:2\n",
      "--------------------------------------------------\n",
      "Query ID: 2\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'18710bc8-2019-04-18T16:37:00Z-00001-000': 3.4603981128831807, '18710bc8-2019-04-18T16:37\n",
      "--------------------------------------------------\n",
      "Query ID: 3\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'82973dd4-2019-04-18T18:46:16Z-00004-000': 3.2892193728350874, '1f173b8c-2019-04-18T18:45\n",
      "--------------------------------------------------\n",
      "Query ID: 4\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'1df1290b-2019-04-19T12:44:36Z-00076-000': 5.246674411330131, '8baf5944-2019-04-18T19:55:\n",
      "--------------------------------------------------\n",
      "Query ID: 5\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'cf4c9cbf-2019-04-17T11:47:24Z-00043-000': 4.277390165265194, 'cf4c9cbf-2019-04-17T11:47:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the number of retrieved documents for the first 5 queries along with their scores\n",
    "print(\"=== Query Results ===\")\n",
    "for query_id, doc_scores in list(bi_encoder_results.items())[:5]:  # Limit to first 5 queries\n",
    "    num_docs = len(doc_scores)  # Number of documents retrieved for this query\n",
    "    print(f\"Query ID: {query_id}\")\n",
    "    print(f\"Number of retrieved documents: {num_docs}\")\n",
    "    print(f\"Scores (truncated): {str(doc_scores)[:90]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0376a3a",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bi-Encoder (E5) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.2941\n",
      "  NDCG@10: 0.2660\n",
      "  NDCG@20: 0.2727\n",
      "  NDCG@30: 0.2976\n",
      "  NDCG@100: 0.3739\n",
      "\n",
      "MAP: 0.1746\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1014\n",
      "  Recall@10: 0.1578\n",
      "  Recall@20: 0.2375\n",
      "  Recall@30: 0.2876\n",
      "  Recall@100: 0.4532\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.2735\n",
      "  P@10    : 0.2224\n",
      "  P@20    : 0.1755\n",
      "  P@30    : 0.1463\n",
      "  P@100   : 0.0722\n",
      "\n",
      "MRR: 0.5491\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Evaluate Bi-Encoder Results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_bi, map_bi, recall_bi, prec_bi, mrr_bi = evaluate_results(qrels, bi_encoder_results, k_values)\n",
    "\n",
    "print(\"=== Bi-Encoder (E5) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_bi)\n",
    "# print(\"MAP:\", map_bi)\n",
    "# print(\"Recall@k:\", recall_bi)\n",
    "# print(\"Precision@k:\", prec_bi)\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_bi.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_bi:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_bi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92333d1d",
   "metadata": {},
   "source": [
    "# Part III: Cross-Encoder Reranking\n",
    "\n",
    "We'll now demonstrate how to:\n",
    "1. Take the **top-k** documents from the **bi-encoder** results.\n",
    "2. Rerank them with a **CrossEncoder**.\n",
    "3. Evaluate again using the same `evaluate_results` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e022653",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder rerank function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define a function to rerank with a CrossEncoder\n",
    "\n",
    "def cross_encode_rerank(cross_encoder_model, \n",
    "                        query_ids_list, query_texts_list,\n",
    "                        topk_results, corpus_text_map,\n",
    "                        rerank_top_k=500, final_top_k=100):\n",
    "    \"\"\"\n",
    "    cross_encoder_model: a CrossEncoder from sentence-transformers\n",
    "    query_ids_list: list of query IDs (strings)\n",
    "    query_texts_list: list of corresponding query texts\n",
    "    topk_results: dict => topk_results[qid][doc_id] = some bi-encoder score\n",
    "    corpus_text_map: a dict doc_id -> doc_text\n",
    "    rerank_top_k: how many documents to consider for reranking from the bi-encoder results\n",
    "    final_top_k: how many documents to keep after cross-encoder reranking\n",
    "\n",
    "    returns: dict => reranked_results[qid][doc_id] = cross-encoder score\n",
    "    \"\"\"\n",
    "    reranked_results = {}\n",
    "\n",
    "    # Use tqdm to show total progress for all queries\n",
    "    with tqdm(total=len(query_ids_list), desc=\"Cross-encoding queries\", unit=\"query\", ncols=80) as pbar:\n",
    "        for idx, qid in enumerate(query_ids_list):\n",
    "            # Get top rerank_top_k document candidates for the query\n",
    "            doc_candidates = sorted(topk_results[qid].items(), key=lambda x: x[1], reverse=True)[:rerank_top_k]\n",
    "            doc_ids = [doc_id for doc_id, _ in doc_candidates]\n",
    "            query_text = query_texts_list[idx]\n",
    "\n",
    "            # Prepare query-document pairs for the cross-encoder\n",
    "#             pair_texts = [(f\"query: {query_text}\", f\"passage: {corpus_text_map[doc_id]}\") for doc_id in doc_ids]\n",
    "            pair_texts = [(query_text, corpus_text_map[doc_id]) for doc_id in doc_ids]\n",
    "            \n",
    "            # Perform cross-encoder inference, ensure no additional progress bars are shown\n",
    "            scores = cross_encoder_model.predict(pair_texts, show_progress_bar=False)\n",
    "\n",
    "            # Combine document IDs with their Cross-Encoder scores\n",
    "            reranked_doc_scores = {doc_ids[i]: float(scores[i]) for i in range(len(doc_ids))}\n",
    "\n",
    "            # Sort reranked results by Cross-Encoder scores and keep final_top_k\n",
    "            reranked_results[qid] = dict(sorted(reranked_doc_scores.items(), key=lambda x: x[1], reverse=True)[:final_top_k])\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "print('Cross-encoder rerank function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc70c7",
   "metadata": {},
   "source": [
    "### Prepare Data Structures for Reranking\n",
    "\n",
    "- We already have `bi_encoder_results`, which is `dict[qid][doc_id] = score`.\n",
    "- We need a quick way to map doc IDs to actual text for the CrossEncoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb81cd38",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id_to_text dictionary has 382545 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Create a dictionary doc_id -> doc_text\n",
    "doc_id_to_text = {}\n",
    "for i, did in enumerate(doc_ids):\n",
    "    doc_id_to_text[did] = doc_texts[i]\n",
    "\n",
    "print(f\"doc_id_to_text dictionary has {len(doc_id_to_text)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951921",
   "metadata": {},
   "source": [
    "## 3.1: MiniLM CrossEncoder Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e642f9",
   "metadata": {},
   "source": [
    "### Load a CrossEncoder\n",
    "Choose any cross-encoder checkpoint from Hugging Face or sentence-transformers.\n",
    "For demonstration, we'll pick something like `'cross-encoder/ms-marco-MiniLM-L-6-v2'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039567e0",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7e047",
   "metadata": {},
   "source": [
    "### Rerank the top-k results from the Bi-Encoder\n",
    "\n",
    "We'll use the cross-encoder function defined above and pass in the top-k results from the Bi-Encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "238a2dd5",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [01:00<00:00,  1.24s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_minilm = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca8420",
   "metadata": {},
   "source": [
    "### Evaluate Cross-Encoder Reranked Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1281eedb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MiniLM Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3659\n",
      "  NDCG@10: 0.3268\n",
      "  NDCG@20: 0.3279\n",
      "  NDCG@30: 0.3441\n",
      "  NDCG@100: 0.4190\n",
      "\n",
      "MAP: 0.2091\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1361\n",
      "  Recall@10: 0.2065\n",
      "  Recall@20: 0.2959\n",
      "  Recall@30: 0.3369\n",
      "  Recall@100: 0.4952\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3918\n",
      "  P@10    : 0.3000\n",
      "  P@20    : 0.2204\n",
      "  P@30    : 0.1714\n",
      "  P@100   : 0.0810\n",
      "\n",
      "MRR: 0.5989\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_minilm, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== MiniLM Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7e3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a532381f",
   "metadata": {},
   "source": [
    "## 3.2: Electra CrossEncoder Reranking\n",
    "To compare the performance of different cross-encoders, here we use `'cross-encoder/ms-marco-electra-base'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "127e8f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-electra-base\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75bcb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [07:28<00:00,  9.15s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_electra = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91efbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Electra Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3689\n",
      "  NDCG@10: 0.3506\n",
      "  NDCG@20: 0.3373\n",
      "  NDCG@30: 0.3553\n",
      "  NDCG@100: 0.4380\n",
      "\n",
      "MAP: 0.2100\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1222\n",
      "  Recall@10: 0.2231\n",
      "  Recall@20: 0.2933\n",
      "  Recall@30: 0.3380\n",
      "  Recall@100: 0.5141\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3714\n",
      "  P@10    : 0.3184\n",
      "  P@20    : 0.2214\n",
      "  P@30    : 0.1741\n",
      "  P@100   : 0.0855\n",
      "\n",
      "MRR: 0.6749\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_electra, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Electra Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe0816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5923b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ed4856",
   "metadata": {},
   "source": [
    "## 3.3: Tiny-BERT CrossEncoder Reranking\n",
    "To compare the performance of different cross-encoders, here we use `'cross-encoder/ms-marco-TinyBERT-L-2-v2'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54220e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab7cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [00:06<00:00,  7.57query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results_tinybert = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c902e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tiny-BERT Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3131\n",
      "  NDCG@10: 0.2998\n",
      "  NDCG@20: 0.3047\n",
      "  NDCG@30: 0.3222\n",
      "  NDCG@100: 0.3981\n",
      "\n",
      "MAP: 0.1848\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1132\n",
      "  Recall@10: 0.1908\n",
      "  Recall@20: 0.2759\n",
      "  Recall@30: 0.3201\n",
      "  Recall@100: 0.4827\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3184\n",
      "  P@10    : 0.2735\n",
      "  P@20    : 0.2061\n",
      "  P@30    : 0.1639\n",
      "  P@100   : 0.0786\n",
      "\n",
      "MRR: 0.5715\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce, mrr_ce = evaluate_results(qrels, cross_encoder_results_tinybert, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Tiny-BERT Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "    \n",
    "print(f\"\\nMRR: {mrr_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2673d8",
   "metadata": {},
   "source": [
    "# Part IV: Evaluating Cross-Encoders Fine-Tuned on Other Datasets\n",
    "This section demonstrates the evaluation of Cross-Encoders fine-tuned on different datasets, applied to the current dataset for relevance scoring. It includes functions to utilize pre-fine-tuned Cross-Encoder models to test their performance on the current dataset. The results provide insights into how well Cross-Encoders fine-tuned on external datasets generalize to new tasks and domains, specifically for reranking tasks or other applications requiring precise query-document matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9211b5",
   "metadata": {},
   "source": [
    "## 4.1 Evaluation of Cross-Encoders Fine-Tuned using Scifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49080b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the cross-encoder fine-tuned using scifact...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [01:00<00:00,  1.24s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the cross-encoder fine-tuned using scifact...\")\n",
    "fine_tuned_model_path = \"./models/scifact_fine_tuned_cross_encoder_minilm\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5809f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scifact Fine-Tuned Cross-Encoder (Mini-LM) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3700\n",
      "  NDCG@10: 0.3428\n",
      "  NDCG@20: 0.3510\n",
      "  NDCG@30: 0.3696\n",
      "  NDCG@100: 0.4306\n",
      "\n",
      "MAP: 0.2166\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1377\n",
      "  Recall@10: 0.2164\n",
      "  Recall@20: 0.3127\n",
      "  Recall@30: 0.3576\n",
      "  Recall@100: 0.4897\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3837\n",
      "  P@10    : 0.3061\n",
      "  P@20    : 0.2367\n",
      "  P@30    : 0.1830\n",
      "  P@100   : 0.0802\n",
      "\n",
      "MRR: 0.5955\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Scifact Fine-Tuned Cross-Encoder (Mini-LM) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f0db58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the cross-encoder fine-tuned using scifact...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [00:06<00:00,  7.37query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the cross-encoder fine-tuned using scifact...\")\n",
    "fine_tuned_model_path = \"./models/scifact_fine_tuned_cross_encoder_tinybert\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4528414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scifact Fine-Tuned Cross-Encoder (Tiny-BERT) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.3658\n",
      "  NDCG@10: 0.3227\n",
      "  NDCG@20: 0.3237\n",
      "  NDCG@30: 0.3481\n",
      "  NDCG@100: 0.4118\n",
      "\n",
      "MAP: 0.1970\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1358\n",
      "  Recall@10: 0.1960\n",
      "  Recall@20: 0.2806\n",
      "  Recall@30: 0.3369\n",
      "  Recall@100: 0.4752\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.3796\n",
      "  P@10    : 0.2837\n",
      "  P@20    : 0.2092\n",
      "  P@30    : 0.1735\n",
      "  P@100   : 0.0780\n",
      "\n",
      "MRR: 0.5869\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== Scifact Fine-Tuned Cross-Encoder (Tiny-BERT) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88370c7",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation of Cross-Encoders Fine-Tuned using NF-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fabb5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the cross-encoder fine-tuned using NF-Corpus...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [01:00<00:00,  1.24s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the cross-encoder fine-tuned using NF-Corpus...\")\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_minilm\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d9980d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NF-Corpus Fine-Tuned Cross-Encoder (MiniLM) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.2871\n",
      "  NDCG@10: 0.2701\n",
      "  NDCG@20: 0.2762\n",
      "  NDCG@30: 0.2976\n",
      "  NDCG@100: 0.3729\n",
      "\n",
      "MAP: 0.1660\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.1043\n",
      "  Recall@10: 0.1721\n",
      "  Recall@20: 0.2449\n",
      "  Recall@30: 0.2943\n",
      "  Recall@100: 0.4551\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.2776\n",
      "  P@10    : 0.2306\n",
      "  P@20    : 0.1745\n",
      "  P@30    : 0.1463\n",
      "  P@100   : 0.0729\n",
      "\n",
      "MRR: 0.5231\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== NF-Corpus Fine-Tuned Cross-Encoder (MiniLM) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16858a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top-k with the cross-encoder fine-tuned using NF-Corpus...\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n",
      "Cross-encoding queries: 100%|████████████████| 49/49 [00:06<00:00,  7.46query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned CrossEncoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Rerank with the Fine-Tuned CrossEncoder\n",
    "#######################################################\n",
    "\n",
    "logging.info(\"Reranking top-k with the cross-encoder fine-tuned using NF-Corpus...\")\n",
    "fine_tuned_model_path = \"./models/nfcorpus_fine_tuned_cross_encoder_tinybert\"\n",
    "fine_tuned_cross_encoder = CrossEncoder(fine_tuned_model_path)\n",
    "\n",
    "rerank_top_k = 300\n",
    "final_top_k = 100\n",
    "\n",
    "cross_encoder_results_finetuned = cross_encode_rerank(\n",
    "    cross_encoder_model=fine_tuned_cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,   # from your bi-encoder retrieval\n",
    "    corpus_text_map=doc_id_to_text,\n",
    "    rerank_top_k=rerank_top_k,\n",
    "    final_top_k=final_top_k\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned CrossEncoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47f84726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NF-Corpus Fine-Tuned Cross-Encoder (Tiny-BERT) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.1844\n",
      "  NDCG@10: 0.1816\n",
      "  NDCG@20: 0.2031\n",
      "  NDCG@30: 0.2269\n",
      "  NDCG@100: 0.3126\n",
      "\n",
      "MAP: 0.1132\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.0579\n",
      "  Recall@10: 0.1206\n",
      "  Recall@20: 0.2015\n",
      "  Recall@30: 0.2503\n",
      "  Recall@100: 0.4326\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.1837\n",
      "  P@10    : 0.1714\n",
      "  P@20    : 0.1439\n",
      "  P@30    : 0.1224\n",
      "  P@100   : 0.0704\n",
      "\n",
      "MRR: 0.4176\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Evaluate the Fine-Tuned CrossEncoder Results\n",
    "#######################################################\n",
    "# 1) Use the same evaluate_results function\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ft, map_ft, recall_ft, prec_ft, mrr_ft = evaluate_results(qrels, cross_encoder_results_finetuned, k_values)\n",
    "\n",
    "# 2) Print or log the metrics\n",
    "print(\"=== NF-Corpus Fine-Tuned Cross-Encoder (Tiny-BERT) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ft.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ft:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ft.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr_ft:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wse-env)",
   "language": "python",
   "name": "wse-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

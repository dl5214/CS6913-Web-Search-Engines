{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb91b09a",
   "metadata": {},
   "source": [
    "# Debugging Bi-Encoder and Cross-Encoder in One Notebook\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load the SciFact dataset (corpus, queries, qrels)\n",
    "2. Perform bi-encoder retrieval using FAISS HNSW.\n",
    "3. Save or inspect intermediate variables (embeddings, top-k results).\n",
    "4. (Optional) Rerank the top-k results with a cross-encoder.\n",
    "5. Compare the evaluations at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbfef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\r\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\r\n",
      "0.00s - to python to disable frozen modules.\r\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\r\n",
      "Available kernels:\r\n",
      "  wse-env    /home/dl5214/.local/share/jupyter/kernels/wse-env\r\n",
      "  python3    /home/shaoyu/anaconda3/share/jupyter/kernels/python3\r\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Select the correct kernel\n",
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc935e3",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dl5214/.conda/envs/wse-env/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Logging\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import faiss\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090e7adc",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading Functions\n",
    "def load_scifact_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact corpus from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of document IDs and document texts.\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    doc_texts = []\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            doc_id = data[\"_id\"]\n",
    "            text_str = data.get(\"text\", \"\")\n",
    "            doc_ids.append(doc_id)\n",
    "            doc_texts.append(text_str)\n",
    "    return doc_ids, doc_texts\n",
    "\n",
    "def load_scifact_queries(queries_file):\n",
    "    \"\"\"\n",
    "    Loads the Scifact queries from a JSONL file.\n",
    "    Each line is a JSON object containing '_id' and 'text'.\n",
    "    Returns lists of query IDs and query texts.\n",
    "    \"\"\"\n",
    "    q_ids = []\n",
    "    q_texts = []\n",
    "    with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            q_ids.append(data[\"_id\"])\n",
    "            q_texts.append(data.get(\"text\", \"\"))\n",
    "    return q_ids, q_texts\n",
    "\n",
    "def load_qrels(qrels_file):\n",
    "    \"\"\"\n",
    "    Loads the qrels from a TSV file.\n",
    "    The first line is a header: 'query-id    corpus-id    score'.\n",
    "    Returns a dictionary: qrels[query_id][doc_id] = relevance_score.\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    with open(qrels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Skip the header line:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            qid, did, rel = line.split()\n",
    "            rel = int(rel)\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][did] = rel\n",
    "    return qrels\n",
    "\n",
    "print('Data loading functions ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e456f24f",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Metric Evaluation Function\n",
    "# We'll define a single function we can call multiple times,\n",
    "# whether for the bi-encoder alone or after cross-encoder reranking.\n",
    "\n",
    "def evaluate_results(qrels, results, k_values=None):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval results using pytrec_eval.\n",
    "    results: dict of dict => results[qid][doc_id] = score\n",
    "    qrels: dict of dict => qrels[qid][doc_id] = relevance\n",
    "    k_values: list of cutoff values (e.g. [5,10,20])\n",
    "    \"\"\"\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"map\", \"ndcg_cut\", \"recall\", \"P\"})\n",
    "    scores = evaluator.evaluate(results)\n",
    "\n",
    "    if k_values is None:\n",
    "        # If k_values not specified, use typical cutoffs\n",
    "        k_values = [5, 10, 20, 30, 100]\n",
    "\n",
    "    ndcg_res = {}\n",
    "    recall_res = {}\n",
    "    prec_res = {}\n",
    "\n",
    "    # Compute mean average precision (MAP) across all queries\n",
    "    map_vals = [scores[qid][\"map\"] for qid in scores]\n",
    "    map_res = np.mean(map_vals)\n",
    "\n",
    "    # Compute NDCG@k, Recall@k, P@k\n",
    "    for k in k_values:\n",
    "        ndcg_vals = []\n",
    "        recall_vals = []\n",
    "        prec_vals = []\n",
    "        for qid in scores:\n",
    "            ndcg_key = f\"ndcg_cut_{k}\"\n",
    "            recall_key = f\"recall_{k}\"\n",
    "            prec_key = f\"P_{k}\"\n",
    "            # If the metric isn't found for some reason, skip\n",
    "            if ndcg_key in scores[qid]:\n",
    "                ndcg_vals.append(scores[qid][ndcg_key])\n",
    "            if recall_key in scores[qid]:\n",
    "                recall_vals.append(scores[qid][recall_key])\n",
    "            if prec_key in scores[qid]:\n",
    "                prec_vals.append(scores[qid][prec_key])\n",
    "\n",
    "        ndcg_res[f\"NDCG@{k}\"] = np.mean(ndcg_vals) if ndcg_vals else 0.0\n",
    "        recall_res[f\"Recall@{k}\"] = np.mean(recall_vals) if recall_vals else 0.0\n",
    "        prec_res[f\"P@{k}\"] = np.mean(prec_vals) if prec_vals else 0.0\n",
    "\n",
    "    return ndcg_res, map_res, recall_res, prec_res\n",
    "\n",
    "print('Evaluation function ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021a8164",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5183 documents and 1109 queries.\n",
      "Unique qrels: 300\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load the SciFact Data\n",
    "corpus_file = \"datasets/scifact/corpus.jsonl\"\n",
    "queries_file = \"datasets/scifact/queries.jsonl\"\n",
    "qrels_file = \"datasets/scifact/qrels/test.tsv\"\n",
    "\n",
    "doc_ids, doc_texts = load_scifact_corpus(corpus_file)\n",
    "query_ids, query_texts = load_scifact_queries(queries_file)\n",
    "qrels = load_qrels(qrels_file)\n",
    "\n",
    "print(f\"Loaded {len(doc_ids)} documents and {len(query_ids)} queries.\")\n",
    "print(f\"Unique qrels: {len(qrels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef59d33",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered queries for evaluation: 300\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Filter queries to those that appear in qrels\n",
    "filtered_query_ids = [qid for qid in query_ids if qid in qrels]\n",
    "filtered_query_texts = [query_texts[query_ids.index(qid)] for qid in filtered_query_ids]\n",
    "\n",
    "print(f\"Filtered queries for evaluation: {len(filtered_query_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1f349",
   "metadata": {},
   "source": [
    "## Bi-Encoder Retrieval with FAISS HNSW\n",
    "\n",
    "We'll:\n",
    "1. Load a SentenceTransformer (E5) model.\n",
    "2. Encode all documents with prefix **\"passage: \"**.\n",
    "3. Build a FAISS HNSW index.\n",
    "4. Encode queries with prefix **\"query: \"**.\n",
    "5. Perform approximate nearest neighbor (ANN) search.\n",
    "6. Evaluate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9c53ed",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/e5-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Bi-Encoder model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Bi-Encoder Setup\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# Set the device to \"cuda\" if available, otherwise fallback to \"cpu\"\n",
    "device = \"cuda\" if cuda_available else \"cpu\"\n",
    "\n",
    "# Load the Bi-Encoder model on the specified device\n",
    "bi_model = SentenceTransformer(\"intfloat/e5-base-v2\", device=device)\n",
    "print(f\"Bi-Encoder model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15acb6a9",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6bb9423d194fe0a8ef51e9d2c64fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus embeddings shape: (5183, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Encode Corpus with E5 (prefix 'passage: ')\n",
    "# We'll store these embeddings in a variable so we can reuse or inspect them.\n",
    "corpus_embeddings = bi_model.encode([\n",
    "    \"passage: \" + text for text in doc_texts\n",
    "],\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(\"Corpus embeddings shape:\", corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2168fbeb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building HNSW index...\n",
      "INFO:root:Moving HNSW index to GPU...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW index ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Build the FAISS HNSW Index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "M = 15  # number of connections per node in HNSW\n",
    "efConstruction = 300\n",
    "efSearch = 1000\n",
    "\n",
    "logging.info(\"Building HNSW index...\")\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, M)\n",
    "index_hnsw.hnsw.efConstruction = efConstruction\n",
    "index_hnsw.hnsw.efSearch = efSearch\n",
    "\n",
    "# Add corpus embeddings to index\n",
    "index_hnsw.add(corpus_embeddings)\n",
    "\n",
    "# If GPU available, move index to GPU\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(\"Moving HNSW index to GPU...\")\n",
    "    faiss_res = faiss.StandardGpuResources()\n",
    "    index_hnsw = faiss.index_cpu_to_gpu(faiss_res, 0, index_hnsw)\n",
    "\n",
    "print(\"HNSW index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275bd6fb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d5daa5c4a64f7295f9f57b431fb433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings shape: (300, 768)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Encode Queries with E5 (prefix 'query: ')\n",
    "# We'll store query embeddings so we can re-run or examine.\n",
    "\n",
    "query_embeddings = bi_model.encode([\n",
    "    \"query: \" + text for text in filtered_query_texts\n",
    "],\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(\"Query embeddings shape:\", query_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36832a0",
   "metadata": {},
   "source": [
    "## Perform ANN Search and Evaluate (Bi-Encoder Only)\n",
    "\n",
    "We'll retrieve the top-k docs from FAISS for each query, then convert them to a `results` dict for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef3f9a84",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching query embeddings in HNSW index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-Encoder top-k results ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Bi-Encoder ANN Search\n",
    "top_k = 500\n",
    "logging.info(\"Searching query embeddings in HNSW index...\")\n",
    "D, I = index_hnsw.search(query_embeddings, top_k)\n",
    "\n",
    "# Convert results to pytrec_eval format\n",
    "# results[qid][docid] = score\n",
    "bi_encoder_results = {}\n",
    "for q_idx, qid in enumerate(filtered_query_ids):\n",
    "    bi_encoder_results[qid] = {}\n",
    "    for rank in range(top_k):\n",
    "        doc_idx = I[q_idx, rank]\n",
    "        # for a distance D[q_idx, rank], we can invert or just use negative distance\n",
    "        # Some prefer 1/dist, or -dist, or use it directly.\n",
    "        # We'll do a small trick: score = 1 / distance.\n",
    "        # If distance is 0, we can handle that or set a high value.\n",
    "        dist = D[q_idx, rank]\n",
    "        score = 1/float(dist) if dist != 0 else 1e9\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        bi_encoder_results[qid][doc_id] = score\n",
    "\n",
    "print(\"Bi-Encoder top-k results ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ad55e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Results ===\n",
      "Query ID: 1\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'17388232': 2.5090898187106117, '4346436': 2.4890392516747983, '40212412': 2.437039325133\n",
      "--------------------------------------------------\n",
      "Query ID: 3\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'19058822': 2.852896374383881, '1388704': 2.5903179370916107, '4414547': 2.56040787747559\n",
      "--------------------------------------------------\n",
      "Query ID: 5\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'13734012': 2.824170426399299, '21550246': 2.606950244671336, '23124332': 2.5345472072412\n",
      "--------------------------------------------------\n",
      "Query ID: 13\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'1263446': 2.8079570087015084, '13791044': 2.7205130071992047, '11748341': 2.710142104149\n",
      "--------------------------------------------------\n",
      "Query ID: 36\n",
      "Number of retrieved documents: 500\n",
      "Scores (truncated): {'37424881': 3.9447428049988513, '18557974': 3.7488652382192527, '33409100': 3.71444588008\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the number of retrieved documents for the first 5 queries along with their scores\n",
    "print(\"=== Query Results ===\")\n",
    "for query_id, doc_scores in list(bi_encoder_results.items())[:5]:  # Limit to first 5 queries\n",
    "    num_docs = len(doc_scores)  # Number of documents retrieved for this query\n",
    "    print(f\"Query ID: {query_id}\")\n",
    "    print(f\"Number of retrieved documents: {num_docs}\")\n",
    "    print(f\"Scores (truncated): {str(doc_scores)[:90]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0376a3a",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bi-Encoder (E5) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.6877\n",
      "  NDCG@10: 0.7121\n",
      "  NDCG@20: 0.7225\n",
      "  NDCG@30: 0.7262\n",
      "  NDCG@100: 0.7343\n",
      "\n",
      "MAP: 0.6704\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.7771\n",
      "  Recall@10: 0.8464\n",
      "  Recall@20: 0.8840\n",
      "  Recall@30: 0.9003\n",
      "  Recall@100: 0.9470\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.1727\n",
      "  P@10    : 0.0950\n",
      "  P@20    : 0.0500\n",
      "  P@30    : 0.0341\n",
      "  P@100   : 0.0107\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Evaluate Bi-Encoder Results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_bi, map_bi, recall_bi, prec_bi = evaluate_results(qrels, bi_encoder_results, k_values)\n",
    "\n",
    "print(\"=== Bi-Encoder (E5) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_bi)\n",
    "# print(\"MAP:\", map_bi)\n",
    "# print(\"Recall@k:\", recall_bi)\n",
    "# print(\"Precision@k:\", prec_bi)\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_bi.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_bi:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_bi.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92333d1d",
   "metadata": {},
   "source": [
    "## Cross-Encoder Reranking\n",
    "\n",
    "We'll now demonstrate how to:\n",
    "1. Take the **top-k** documents from the **bi-encoder** results.\n",
    "2. Rerank them with a **CrossEncoder**.\n",
    "3. Evaluate again using the same `evaluate_results` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e022653",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder rerank function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define a function to rerank with a CrossEncoder\n",
    "\n",
    "# # print('Cross-encoder rerank function defined.')\n",
    "# def cross_encode_rerank(cross_encoder_model, \n",
    "#                         query_ids_list, query_texts_list,\n",
    "#                         topk_results, corpus_text_map,\n",
    "#                         top_k=100):\n",
    "#     \"\"\"\n",
    "#     cross_encoder_model: a CrossEncoder from sentence-transformers\n",
    "#     query_ids_list: list of query IDs (strings)\n",
    "#     query_texts_list: list of corresponding query texts\n",
    "#     topk_results: dict => topk_results[qid][doc_id] = some bi-encoder score\n",
    "#     corpus_text_map: a dict doc_id -> doc_text\n",
    "#     top_k: how many docs we have from the bi-encoder stage (and will re-rank)\n",
    "\n",
    "#     returns: dict => reranked_results[qid][doc_id] = cross-encoder score\n",
    "#     \"\"\"\n",
    "#     reranked_results = {}\n",
    "\n",
    "#     # Use tqdm to show total progress for all queries\n",
    "#     with tqdm(total=len(query_ids_list), desc=\"Cross-encoding queries\", unit=\"query\", ncols=80) as pbar:\n",
    "#         for idx, qid in enumerate(query_ids_list):\n",
    "#             # Get top-k document candidates for the query\n",
    "#             doc_candidates = list(topk_results[qid].keys())[:top_k]\n",
    "#             query_text = query_texts_list[idx]\n",
    "\n",
    "#             # Prepare query-document pairs for the cross-encoder\n",
    "#             pair_texts = [(f\"query: {query_text}\", f\"passage: {corpus_text_map[did]}\") for did in doc_candidates]\n",
    "\n",
    "#             # Perform cross-encoder inference, ensure no additional progress bars are shown\n",
    "#             scores = cross_encoder_model.predict(pair_texts, show_progress_bar=False)\n",
    "\n",
    "#             # Store scores in the output dictionary\n",
    "#             doc_scores = {doc_candidates[i]: float(scores[i]) for i in range(len(doc_candidates))}\n",
    "#             reranked_results[qid] = doc_scores\n",
    "\n",
    "#             # Update the progress bar\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     return reranked_results\n",
    "\n",
    "def cross_encode_rerank(cross_encoder_model, \n",
    "                        query_ids_list, query_texts_list,\n",
    "                        topk_results, corpus_text_map,\n",
    "                        rerank_top_k=500, final_top_k=100):\n",
    "    \"\"\"\n",
    "    cross_encoder_model: a CrossEncoder from sentence-transformers\n",
    "    query_ids_list: list of query IDs (strings)\n",
    "    query_texts_list: list of corresponding query texts\n",
    "    topk_results: dict => topk_results[qid][doc_id] = some bi-encoder score\n",
    "    corpus_text_map: a dict doc_id -> doc_text\n",
    "    rerank_top_k: how many documents to consider for reranking from the bi-encoder results\n",
    "    final_top_k: how many documents to keep after cross-encoder reranking\n",
    "\n",
    "    returns: dict => reranked_results[qid][doc_id] = cross-encoder score\n",
    "    \"\"\"\n",
    "    reranked_results = {}\n",
    "\n",
    "    # Use tqdm to show total progress for all queries\n",
    "    with tqdm(total=len(query_ids_list), desc=\"Cross-encoding queries\", unit=\"query\", ncols=80) as pbar:\n",
    "        for idx, qid in enumerate(query_ids_list):\n",
    "            # Get top rerank_top_k document candidates for the query\n",
    "            doc_candidates = sorted(topk_results[qid].items(), key=lambda x: x[1], reverse=True)[:rerank_top_k]\n",
    "            doc_ids = [doc_id for doc_id, _ in doc_candidates]\n",
    "            query_text = query_texts_list[idx]\n",
    "\n",
    "            # Prepare query-document pairs for the cross-encoder\n",
    "            pair_texts = [(f\"query: {query_text}\", f\"passage: {corpus_text_map[doc_id]}\") for doc_id in doc_ids]\n",
    "\n",
    "            # Perform cross-encoder inference, ensure no additional progress bars are shown\n",
    "            scores = cross_encoder_model.predict(pair_texts, show_progress_bar=False)\n",
    "\n",
    "            # Combine document IDs with their Cross-Encoder scores\n",
    "            reranked_doc_scores = {doc_ids[i]: float(scores[i]) for i in range(len(doc_ids))}\n",
    "\n",
    "            # Sort reranked results by Cross-Encoder scores and keep final_top_k\n",
    "            reranked_results[qid] = dict(sorted(reranked_doc_scores.items(), key=lambda x: x[1], reverse=True)[:final_top_k])\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "print('Cross-encoder rerank function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc70c7",
   "metadata": {},
   "source": [
    "### Prepare Data Structures for Reranking\n",
    "\n",
    "- We already have `bi_encoder_results`, which is `dict[qid][doc_id] = score`.\n",
    "- We need a quick way to map doc IDs to actual text for the CrossEncoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb81cd38",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id_to_text dictionary has 5183 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Create a dictionary doc_id -> doc_text\n",
    "doc_id_to_text = {}\n",
    "for i, did in enumerate(doc_ids):\n",
    "    doc_id_to_text[did] = doc_texts[i]\n",
    "\n",
    "print(f\"doc_id_to_text dictionary has {len(doc_id_to_text)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e642f9",
   "metadata": {},
   "source": [
    "### Load a CrossEncoder\n",
    "Choose any cross-encoder checkpoint from Hugging Face or sentence-transformers.\n",
    "For demonstration, we'll pick something like `'cross-encoder/ms-marco-MiniLM-L-6-v2'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "039567e0",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7e047",
   "metadata": {},
   "source": [
    "### Rerank the top-k results from the Bi-Encoder\n",
    "\n",
    "We'll use the cross-encoder function defined above and pass in the top-k results from the Bi-Encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "238a2dd5",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries: 100%|██████████████| 300/300 [06:11<00:00,  1.24s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca8420",
   "metadata": {},
   "source": [
    "### Evaluate Cross-Encoder Reranked Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1281eedb",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cross-Encoder (Rerank) Results ===\n",
      "NDCG@k:\n",
      "  NDCG@5 : 0.6368\n",
      "  NDCG@10: 0.6641\n",
      "  NDCG@20: 0.6761\n",
      "  NDCG@30: 0.6851\n",
      "  NDCG@100: 0.6973\n",
      "\n",
      "MAP: 0.6194\n",
      "\n",
      "Recall@k:\n",
      "  Recall@5: 0.7284\n",
      "  Recall@10: 0.8072\n",
      "  Recall@20: 0.8519\n",
      "  Recall@30: 0.8930\n",
      "  Recall@100: 0.9593\n",
      "\n",
      "Precision@k:\n",
      "  P@5     : 0.1600\n",
      "  P@10    : 0.0903\n",
      "  P@20    : 0.0480\n",
      "  P@30    : 0.0336\n",
      "  P@100   : 0.0109\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce = evaluate_results(qrels, cross_encoder_results, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a13b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7261b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e728d7e575d4e578babd43d1ecb9d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58a8abc60444505936a1540f4e1c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6723f8174840039c8eaac0beabf54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b124946e901f436f93ba68d823e62639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f0672692d543e3a5b6bb9104433e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: CrossEncoder initialization\n",
    "cross_model_name = \"cross-encoder/ms-marco-electra-base\"  # or any other suitable model\n",
    "cross_encoder = CrossEncoder(cross_model_name, device=device)\n",
    "print(\"CrossEncoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reranking top 300 candidates with cross-encoder, keeping top 100...\n",
      "Cross-encoding queries:  11%|█▋             | 34/300 [05:01<40:23,  9.11s/query]"
     ]
    }
   ],
   "source": [
    "# Cell 15: Rerank top-k with CrossEncoder\n",
    "\n",
    "# Parameters to control the number of documents to rerank and return\n",
    "rerank_top_k = 300  # Number of documents to consider for reranking\n",
    "final_top_k = 100    # Number of documents to keep after reranking\n",
    "\n",
    "logging.info(f\"Reranking top {rerank_top_k} candidates with cross-encoder, keeping top {final_top_k}...\")\n",
    "\n",
    "cross_encoder_results = cross_encode_rerank(\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    query_ids_list=filtered_query_ids,\n",
    "    query_texts_list=filtered_query_texts,\n",
    "    topk_results=bi_encoder_results,  # Bi-Encoder results\n",
    "    corpus_text_map=doc_id_to_text,   # Document text mapping\n",
    "    rerank_top_k=rerank_top_k,        # Candidates to rerank\n",
    "    final_top_k=final_top_k           # Final results to keep\n",
    ")\n",
    "\n",
    "print(\"Cross-encoder reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Evaluate the cross-encoder-based results\n",
    "k_values = [5, 10, 20, 30, 100]\n",
    "ndcg_ce, map_ce, recall_ce, prec_ce = evaluate_results(qrels, cross_encoder_results, k_values)\n",
    "\n",
    "# print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "# print(\"NDCG@k:\", ndcg_ce)\n",
    "# print(\"MAP:\", map_ce)\n",
    "# print(\"Recall@k:\", recall_ce)\n",
    "# print(\"Precision@k:\", prec_ce)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"=== Cross-Encoder (Rerank) Results ===\")\n",
    "print(\"NDCG@k:\")\n",
    "for k, v in ndcg_ce.items():\n",
    "    print(f\"  {k:<7}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nMAP: {map_ce:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@k:\")\n",
    "for k, v in recall_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@k:\")\n",
    "for k, v in prec_ce.items():\n",
    "    print(f\"  {k:<8}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wse-env)",
   "language": "python",
   "name": "wse-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
